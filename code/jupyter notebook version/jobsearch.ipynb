{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we load the required libraries\n",
    "\n",
    "from cgitb import text\n",
    "from dataclasses import replace\n",
    "from gettext import gettext\n",
    "from msilib.schema import Class\n",
    "from operator import contains, countOf\n",
    "from os import getcwd, link\n",
    "from pickle import TRUE\n",
    "from typing import Mapping\n",
    "from unittest import result\n",
    "from xml.dom.minidom import Element\n",
    "from attr import attr, attrs\n",
    "from pyparsing import line\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "#import time\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------#\n",
    "#---Programm command lines for web scraping on Indeed----#\n",
    "#--------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#here we load the browser and the indeed website\n",
    "browser=webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "browser.get(\"https://ca.indeed.com/\")\n",
    "time.sleep(2)   #this command force the programm to wait 2 seconds to make sure the hole page has been loaded \n",
    "                #before executing the next commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear the search boxes (job title and location) on the web page\n",
    "browser.find_element(By.ID, 'text-input-what').click()\n",
    "try: browser.find_element(By.XPATH, '//input[@ID=\"text-input-what\"]/following::span').click()\n",
    "except: pass\n",
    "browser.find_element(By.ID, 'text-input-where').click()\n",
    "try: browser.find_element(By.XPATH, '//input[@ID=\"text-input-where\"]/following::span').click()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search according to the user preferences of job and location\n",
    "search_name = input('What job are you looking for?')    #request for the job title the user wants to search for\n",
    "browser.find_element(By.ID, 'text-input-what').send_keys(search_name)      #paste the job title to the box\n",
    "\n",
    "search_place = input('In what location do yo want to look for the job?')       #request for the job location the user wants to search for\n",
    "browser.find_element(By.ID, 'text-input-where').send_keys(search_place)     #paste the job location to the box\n",
    "\n",
    "browser.find_element(By.XPATH, '//button[@class=\"yosegi-InlineWhatWhere-primaryButton\"]').click()   #search for the results\n",
    "time.sleep(1)\n",
    "\n",
    "try: browser.find_element(By.XPATH, '//*[@id=\"popover-x\"]/button').click()       #close popup window\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will return the path to a specific page on indeed\n",
    "def test(n):\n",
    "    next_path = f'//a[@aria-label=\"{n}\"]'   #path structure is obtained through the website html inspection\n",
    "    return next_path\n",
    "\n",
    "#this function return the end of url for each page\n",
    "def end_url(x):\n",
    "    if x > 1:\n",
    "        a = x - 1\n",
    "        end = '&start=' + f'{a}0'\n",
    "    else:\n",
    "        end = \"\"       #first page has no ending\n",
    "    return end\n",
    "\n",
    "# function to extract html document from given url\n",
    "def getHTML(url):\n",
    "    # request for HTML document of given url\n",
    "    response = requests.get(url, headers={\"Content-Type\":\"text\"})\n",
    "    # response will be provided in JSON format\n",
    "    return response.text\n",
    "\n",
    "# this function estimates the time necessary for downloading data\n",
    "def delay(nb):\n",
    "    if nb/20 < 1:\n",
    "        delay = f'{int(nb/20)*60} sec'\n",
    "    elif nb/20 >= 1 and nb/20 < 60:\n",
    "        delay = f'{int(nb/20)} mns'\n",
    "    else: delay = f'{int(nb/20/60)} hrs {int(((nb/20/60)-int(nb/20/60))*60)} mns'\n",
    "    return delay\n",
    "\n",
    "#this function inform on the occurence of a specific string in the job description section\n",
    "def skill_test(x):\n",
    "    result = 'check website'\n",
    "    try:\n",
    "        string = sub_soup.find('div', attrs={'id':'jobDescriptionText'}).text\n",
    "        result_list = []\n",
    "        for each in x:        \n",
    "            result_list.append(each in string)  # append True/False for each element in substring\n",
    "        r = any(result_list) #call any() with boolean results list\n",
    "        if r == True:\n",
    "            result = 'yes'\n",
    "        else:\n",
    "            result = 'no'\n",
    "    except: pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data on 1 jobs will be downloaded. Estimated time is 0 sec.\n"
     ]
    }
   ],
   "source": [
    "#estimate the time for dowloading data\n",
    "nb = int(browser.find_element(By.ID, 'searchCountPages').text.split()[3])\n",
    "print(f'Data on {nb} jobs will be downloaded. Estimated time is {delay(nb)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [TITLE, COMPANY, LOCATION, T_WORK, T_CONTRACT, SALARY, EXCEL, PYTHON, R, SAS, STATA, VBA, SQL, POWER_BI, WEBSITE]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#next lines allow to navigates through all the pages containing job announcement\n",
    "\n",
    "i = 0\n",
    "main_url = browser.current_url\n",
    "jobtab = pd.DataFrame(columns=['TITLE', 'COMPANY', 'LOCATION', 'T_WORK', 'T_CONTRACT', 'SALARY', 'EXCEL', 'PYTHON', 'R', 'SAS', 'STATA', 'VBA', 'SQL', 'POWER_BI', 'WEBSITE'])\n",
    "\n",
    "while i == 0:\n",
    "\n",
    "    main_soup = BeautifulSoup(getHTML(main_url), \"html.parser\")\n",
    "    time.sleep(2)  \n",
    "    for each in main_soup.find_all('a', attrs={'class':'jcs-JobTitle css-jspxzf eu4oa1w0'}):\n",
    "        sub_url = 'https://ca.indeed.com' + each['href']\n",
    "        sub_soup = BeautifulSoup(getHTML(sub_url), \"html.parser\")\n",
    "        time.sleep(2)\n",
    "        dom = etree.HTML(str(sub_soup))\n",
    "        time.sleep(2)\n",
    "        \n",
    "        #Commands to report data from web pages to database\n",
    "        title = ''\n",
    "        comp_name = ''\n",
    "        location = ''\n",
    "        t_work = ''\n",
    "        t_contract =''\n",
    "        salary = ''\n",
    "        website = ''\n",
    "\n",
    "        try:\n",
    "            title = sub_soup.find('h1').text     #title of the post\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "            comp_name = dom.xpath(\"//div[@class='jobsearch-InlineCompanyRating icl-u-xs-mt--xs jobsearch-DesktopStickyContainer-companyrating']//a\")[0].text    #name of the company\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "            location = dom.xpath('//div[@class=\"icl-u-xs-mt--xs icl-u-textColor--secondary jobsearch-JobInfoHeader-subtitle jobsearch-DesktopStickyContainer-subtitle\"]//div[contains(text(),\"QC\")]')[0].text.replace(', QC','')\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            t_work = dom.xpath('//div[@class=\"icl-u-xs-mt--xs icl-u-textColor--secondary jobsearch-JobInfoHeader-subtitle jobsearch-DesktopStickyContainer-subtitle\"]//div[contains(text(),\"QC\")]/../following-sibling::div/div')[0].text\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "            t_contract = sub_soup.find('span', attrs={'class':'jobsearch-JobMetadataHeader-item icl-u-xs-mt--xs'}).text      #type of contract (full time/part time, casual/permanent)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            salary = sub_soup.find('span', attrs={'class':'icl-u-xs-mr--xs attribute_snippet'}).text      #salary\n",
    "        except: pass\n",
    "\n",
    "        excel = skill_test(['Excel', 'excel', 'EXCEL', 'MSExcel', 'MSexcel'])   #check if SAS skill is required\n",
    "        python = skill_test(['Python', 'python', 'PYTHON'])   #check if SAS skill is required\n",
    "        r = skill_test(['R,', 'R.', 'R)'])   #check if R skill is required\n",
    "        sas = skill_test(['SAS'])   #check if SAS skill is required\n",
    "        stata = skill_test(['Stata', 'stata', 'STATA'])   #check if SAS skill is required\n",
    "        vba = skill_test(['VBA'])   #check if SAS skill is required\n",
    "        sql = skill_test(['SQL'])   #check if SQL skill is required\n",
    "        power_bi = skill_test(['Power BI', 'PowerBI', 'powerBI', 'power BI', 'BI visualization'])       #check if Power BI skill is required\n",
    "\n",
    "                                                        #set the new line of observation       \n",
    "        obs = pd.DataFrame([[title, comp_name, location, t_work, t_contract, salary, excel, python, r, sas, stata, vba, sql, power_bi, sub_url]], columns=['TITLE', 'COMPANY', 'LOCATION', 'T_WORK', 'T_CONTRACT', 'SALARY', 'EXCEL', 'PYTHON', 'R', 'SAS', 'STATA', 'VBA', 'SQL', 'POWER_BI', 'WEBSITE'])\n",
    "        \n",
    "        jobtab = pd.concat([jobtab, obs], axis=0)       #adding the new observation to the table\n",
    "\n",
    "    try:\n",
    "        main_url = browser.find_element(By.XPATH, '//head/link[@rel=\"next\"]').get_attribute('href')\n",
    "        browser.get(main_url)\n",
    "    except NoSuchElementException:\n",
    "        i = 1\n",
    "\n",
    "print(jobtab)\n",
    "\n",
    "try:\n",
    "    deljobs = browser.find_element(By.CLASS_NAME, 'dupetext').text.split()[3]\n",
    "    print(f'Note: {deljobs} jobs have been removed to avoid duplicated results')\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtab.to_csv('draft/jobs.csv', index=False, encoding='UTF-8', sep=';')       #save the database to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(['http://google.com', 'http://duckduckgo.com'])\n",
    "\n",
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val, 'val')\n",
    "\n",
    "df.style.format(make_clickable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = browser.current_url\n",
    "main_soup = BeautifulSoup(getHTML(main_url), \"html.parser\")\n",
    "main_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_state(x):\n",
    "    for each in x:\n",
    "        states = states + 'contains(text(),\"{x}\")'\n",
    "    return states\n",
    "\n",
    "can_states = [\"AB\", \"BC\", \"PE\", \"MB\", \"NB\", \"NS\", \"NU\", \"ON\", \"QC\", \"SK\", \"NL\", \"NT\", \"YT\"]\n",
    "\n",
    "test_state(can_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
